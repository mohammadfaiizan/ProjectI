OPTIMIZATION THEORY - UNCONSTRAINED OPTIMIZATION
=================================================

TABLE OF CONTENTS:
1. Fundamentals of Optimization Theory
2. Necessary and Sufficient Conditions
3. Gradient Descent Methods
4. Newton's Method and Quasi-Newton Methods
5. Line Search Techniques
6. Convergence Analysis
7. Stochastic Optimization
8. Applications in Machine Learning

=======================================================

1. FUNDAMENTALS OF OPTIMIZATION THEORY
======================================

1.1 Problem Formulation:
-----------------------
Unconstrained optimization problem:
minimize f(x) where x ‚àà ‚Ñù‚Åø

Components:
- Objective function: f: ‚Ñù‚Åø ‚Üí ‚Ñù
- Decision variables: x = [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]·µÄ
- Optimal solution: x* = argmin f(x)
- Optimal value: f* = f(x*)

1.2 Types of Optima:
-------------------
Global minimum: f(x*) ‚â§ f(x) for all x ‚àà ‚Ñù‚Åø
Local minimum: f(x*) ‚â§ f(x) for all x in neighborhood of x*
Strict local minimum: f(x*) < f(x) for all x ‚â† x* in neighborhood

Isolated minimum: x* has neighborhood containing no other local minima
Weak minimum: f(x*) = f(x) for some x ‚â† x* in neighborhood

1.3 Convexity:
-------------
Convex function: f(Œªx + (1-Œª)y) ‚â§ Œªf(x) + (1-Œª)f(y) for all Œª ‚àà [0,1]

Properties:
- Any local minimum is global minimum
- Set of global minima is convex
- Gradient condition: f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y - x)

Strictly convex: Inequality is strict for x ‚â† y and Œª ‚àà (0,1)
- Implies unique global minimum

Strong convexity: f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y - x) + (Œº/2)||y - x||¬≤
- Parameter Œº > 0 is strong convexity constant
- Implies faster convergence rates

1.4 Smoothness:
--------------
Lipschitz continuous gradient: ||‚àáf(x) - ‚àáf(y)|| ‚â§ L||x - y||
- L is Lipschitz constant
- Enables convergence analysis with fixed step sizes

Twice differentiable: Second derivatives exist and are continuous
- Enables Newton-type methods
- Hessian matrix: H(x) = ‚àá¬≤f(x)

=======================================================

2. NECESSARY AND SUFFICIENT CONDITIONS
======================================

2.1 First-Order Necessary Conditions:
-------------------------------------
Theorem: If x* is local minimum and f differentiable at x*, then:
‚àáf(x*) = 0

Proof outline:
- If ‚àáf(x*) ‚â† 0, then d = -‚àáf(x*) is descent direction
- For small Œ± > 0: f(x* + Œ±d) < f(x*)
- Contradicts local minimality

Critical point: Point where ‚àáf(x) = 0
- All local minima are critical points
- Not all critical points are local minima

2.2 Second-Order Necessary Conditions:
-------------------------------------
Theorem: If x* is local minimum and f twice differentiable at x*, then:
1. ‚àáf(x*) = 0 (first-order condition)
2. ‚àá¬≤f(x*) ‚™∞ 0 (Hessian positive semidefinite)

Implications:
- All eigenvalues of Hessian ‚â• 0
- May have zero eigenvalues (degenerate case)

2.3 Second-Order Sufficient Conditions:
--------------------------------------
Theorem: If f twice continuously differentiable and:
1. ‚àáf(x*) = 0
2. ‚àá¬≤f(x*) ‚âª 0 (Hessian positive definite)

Then x* is strict local minimum.

Implications:
- All eigenvalues of Hessian > 0
- Guarantees isolated local minimum
- Enables convergence analysis for Newton's method

2.4 Saddle Points:
-----------------
Critical point that is neither local minimum nor maximum
- Hessian has both positive and negative eigenvalues
- Common in high-dimensional optimization
- Challenge for gradient-based methods

Escape from saddle points:
- Gradient descent with noise
- Trust region methods
- Second-order information

=======================================================

3. GRADIENT DESCENT METHODS
===========================

3.1 Basic Gradient Descent:
---------------------------
Algorithm:
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚Çñ‚àáf(x‚Çñ)

where Œ±‚Çñ > 0 is step size (learning rate)

Intuition:
- Move in direction of steepest descent
- ‚àáf(x‚Çñ) points in direction of steepest increase
- -‚àáf(x‚Çñ) points in direction of steepest decrease

3.2 Step Size Selection:
-----------------------
Fixed step size: Œ±‚Çñ = Œ± for all k
- Simple but may not converge or converge slowly
- Requires careful tuning

Diminishing step size: Œ±‚Çñ ‚Üí 0 as k ‚Üí ‚àû
- Conditions: Œ£‚Çñ Œ±‚Çñ = ‚àû, Œ£‚Çñ Œ±‚Çñ¬≤ < ‚àû
- Example: Œ±‚Çñ = 1/k, Œ±‚Çñ = 1/‚àök

Exact line search: Œ±‚Çñ = argmin_Œ± f(x‚Çñ - Œ±‚àáf(x‚Çñ))
- Optimal step size in gradient direction
- Often computationally expensive

Backtracking line search: Start with Œ±‚ÇÄ, reduce until sufficient decrease
- Armijo condition: f(x‚Çñ‚Çä‚ÇÅ) ‚â§ f(x‚Çñ) - c‚ÇÅŒ±‚Çñ||‚àáf(x‚Çñ)||¬≤
- Balances computational cost and convergence

3.3 Convergence Analysis:
------------------------
For convex f with Lipschitz continuous gradient:
f(x‚Çñ) - f* ‚â§ O(1/k)

For strongly convex f:
||x‚Çñ - x*||¬≤ ‚â§ (1 - Œº/L)·µè||x‚ÇÄ - x*||¬≤

where Œº is strong convexity parameter, L is Lipschitz constant

Condition number: Œ∫ = L/Œº
- Œ∫ ‚âà 1: Fast convergence
- Œ∫ >> 1: Slow convergence ("ill-conditioned")

3.4 Momentum Methods:
--------------------
Heavy ball method:
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚àáf(x‚Çñ) + Œ≤(x‚Çñ - x‚Çñ‚Çã‚ÇÅ)

Intuition: Add momentum to accelerate convergence
- Œ≤ ‚àà [0,1) is momentum parameter
- Helps escape poor local geometry

Nesterov accelerated gradient:
y‚Çñ = x‚Çñ + Œ≤(x‚Çñ - x‚Çñ‚Çã‚ÇÅ)
x‚Çñ‚Çä‚ÇÅ = y‚Çñ - Œ±‚àáf(y‚Çñ)

Achieves optimal O(1/k¬≤) convergence rate for convex functions

=======================================================

4. NEWTON'S METHOD AND QUASI-NEWTON METHODS
===========================================

4.1 Newton's Method:
-------------------
Algorithm:
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - [‚àá¬≤f(x‚Çñ)]‚Åª¬π‚àáf(x‚Çñ)

Derivation from Taylor expansion:
f(x) ‚âà f(x‚Çñ) + ‚àáf(x‚Çñ)·µÄ(x - x‚Çñ) + ¬Ω(x - x‚Çñ)·µÄ‚àá¬≤f(x‚Çñ)(x - x‚Çñ)

Minimize quadratic approximation:
‚àáf(x‚Çñ) + ‚àá¬≤f(x‚Çñ)(x - x‚Çñ) = 0

Newton direction: d‚Çñ = -[‚àá¬≤f(x‚Çñ)]‚Åª¬π‚àáf(x‚Çñ)

4.2 Properties of Newton's Method:
---------------------------------
Advantages:
- Quadratic convergence near solution
- Scale invariant (invariant to linear transformations)
- Uses second-order information

Disadvantages:
- Requires Hessian computation and inversion: O(n¬≥) per iteration
- Hessian may not be positive definite
- May not converge globally

Convergence rate:
||x‚Çñ‚Çä‚ÇÅ - x*|| ‚â§ M||x‚Çñ - x*||¬≤

for some constant M when close to solution

4.3 Modified Newton Methods:
---------------------------
Damped Newton: Add line search
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚Çñ[‚àá¬≤f(x‚Çñ)]‚Åª¬π‚àáf(x‚Çñ)

Regularized Newton: Ensure positive definiteness
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - [‚àá¬≤f(x‚Çñ) + Œª‚ÇñI]‚Åª¬π‚àáf(x‚Çñ)

where Œª‚Çñ ‚â• 0 is regularization parameter

Trust region: Constrain step size
minimize m‚Çñ(p) = ‚àáf(x‚Çñ)·µÄp + ¬Ωp·µÄ‚àá¬≤f(x‚Çñ)p
subject to ||p|| ‚â§ Œî‚Çñ

4.4 Quasi-Newton Methods:
------------------------
Idea: Approximate Hessian using gradient information only

Secant condition: B‚Çñ‚Çä‚ÇÅs‚Çñ = y‚Çñ
where s‚Çñ = x‚Çñ‚Çä‚ÇÅ - x‚Çñ, y‚Çñ = ‚àáf(x‚Çñ‚Çä‚ÇÅ) - ‚àáf(x‚Çñ)

BFGS update:
B‚Çñ‚Çä‚ÇÅ = B‚Çñ - (B‚Çñs‚Çñs‚Çñ·µÄB‚Çñ)/(s‚Çñ·µÄB‚Çñs‚Çñ) + (y‚Çñy‚Çñ·µÄ)/(y‚Çñ·µÄs‚Çñ)

L-BFGS: Limited memory version
- Store only last m vector pairs (s·µ¢, y·µ¢)
- O(mn) storage instead of O(n¬≤)
- Efficient for large-scale problems

DFP update: Dual to BFGS
- Updates inverse Hessian approximation directly
- Generally less robust than BFGS

=======================================================

5. LINE SEARCH TECHNIQUES
=========================

5.1 Exact Line Search:
---------------------
Problem: Œ±‚Çñ = argmin_Œ± f(x‚Çñ + Œ±d‚Çñ)

where d‚Çñ is search direction

Optimality condition: ‚àáf(x‚Çñ + Œ±‚Çñd‚Çñ)·µÄd‚Çñ = 0
- Gradient orthogonal to search direction
- Often expensive to compute exactly

5.2 Wolfe Conditions:
--------------------
Sufficient decrease (Armijo condition):
f(x‚Çñ + Œ±d‚Çñ) ‚â§ f(x‚Çñ) + c‚ÇÅŒ±‚àáf(x‚Çñ)·µÄd‚Çñ

Curvature condition:
‚àáf(x‚Çñ + Œ±d‚Çñ)·µÄd‚Çñ ‚â• c‚ÇÇ‚àáf(x‚Çñ)·µÄd‚Çñ

where 0 < c‚ÇÅ < c‚ÇÇ < 1 (typically c‚ÇÅ = 10‚Åª‚Å¥, c‚ÇÇ = 0.9)

Strong Wolfe conditions: Replace ‚â• with |‚àáf(x‚Çñ + Œ±d‚Çñ)·µÄd‚Çñ| ‚â§ c‚ÇÇ|‚àáf(x‚Çñ)·µÄd‚Çñ|

5.3 Backtracking Line Search:
----------------------------
Algorithm:
1. Start with Œ± = Œ±‚ÇÄ
2. While f(x‚Çñ + Œ±d‚Çñ) > f(x‚Çñ) + c‚ÇÅŒ±‚àáf(x‚Çñ)·µÄd‚Çñ:
   Œ± = œÅŒ± (typically œÅ = 0.5)
3. Return Œ±

Simple and efficient for many applications
Guarantees sufficient decrease

5.4 Interpolation Methods:
-------------------------
Quadratic interpolation:
- Use function values at 3 points
- Fit quadratic polynomial
- Minimize to find step size

Cubic interpolation:
- Use function and gradient values
- More accurate but more expensive

Bisection method:
- For unimodal functions
- Guaranteed convergence
- Slower than interpolation methods

=======================================================

6. CONVERGENCE ANALYSIS
=======================

6.1 Convergence Rates:
---------------------
Linear convergence: ||x‚Çñ‚Çä‚ÇÅ - x*|| ‚â§ r||x‚Çñ - x*||
- Constant r ‚àà (0,1)
- Error decreases exponentially: O(r·µè)

Superlinear convergence: ||x‚Çñ‚Çä‚ÇÅ - x*||/||x‚Çñ - x*|| ‚Üí 0
- Faster than any linear rate
- BFGS often achieves superlinear convergence

Quadratic convergence: ||x‚Çñ‚Çä‚ÇÅ - x*|| ‚â§ M||x‚Çñ - x*||¬≤
- Newton's method near solution
- Very fast convergence when applicable

6.2 Global vs Local Convergence:
-------------------------------
Global convergence: Algorithm converges from any starting point
- Usually to stationary point, not necessarily global minimum
- Requires line search or trust region

Local convergence: Algorithm converges when started near solution
- Often faster convergence rates
- Newton's method, BFGS with exact line search

6.3 Complexity Analysis:
-----------------------
Oracle complexity: Number of function/gradient evaluations
Iteration complexity: Number of iterations
Arithmetic complexity: Total computational cost

For convex smooth functions:
- Gradient descent: O(1/Œµ) iterations for Œµ-optimal solution
- Accelerated methods: O(1/‚àöŒµ) iterations
- Newton's method: O(log log(1/Œµ)) iterations (if feasible)

6.4 Stopping Criteria:
---------------------
Gradient norm: ||‚àáf(x‚Çñ)|| ‚â§ Œµ
Relative gradient: ||‚àáf(x‚Çñ)||/||‚àáf(x‚ÇÄ)|| ‚â§ Œµ
Function change: |f(x‚Çñ‚Çä‚ÇÅ) - f(x‚Çñ)| ‚â§ Œµ
Relative function change: |f(x‚Çñ‚Çä‚ÇÅ) - f(x‚Çñ)|/|f(x‚Çñ)| ‚â§ Œµ
Parameter change: ||x‚Çñ‚Çä‚ÇÅ - x‚Çñ|| ‚â§ Œµ

Practical considerations:
- Combine multiple criteria
- Account for numerical precision
- Problem-specific tolerances

=======================================================

7. STOCHASTIC OPTIMIZATION
==========================

7.1 Stochastic Gradient Descent (SGD):
--------------------------------------
Problem: minimize f(x) = ùîº[F(x, Œæ)]

where Œæ is random variable

Algorithm:
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±‚Çñg‚Çñ

where g‚Çñ is stochastic gradient: ùîº[g‚Çñ] = ‚àáf(x‚Çñ)

Examples:
- Machine learning: F(x, Œæ) = loss on single data point
- Simulation optimization: F(x, Œæ) = simulation output

7.2 Variance and Bias:
---------------------
Stochastic gradient g‚Çñ = ‚àáF(x‚Çñ, Œæ‚Çñ)
- Unbiased: ùîº[g‚Çñ] = ‚àáf(x‚Çñ)
- Variance: Var[g‚Çñ] = ùîº[||g‚Çñ - ‚àáf(x‚Çñ)||¬≤]

High variance causes:
- Slow convergence
- Oscillations around optimum
- Requires diminishing step sizes

Variance reduction techniques:
- Mini-batching
- Control variates
- Importance sampling

7.3 Mini-Batch Methods:
----------------------
Batch size b: Use b samples per iteration
g‚Çñ = (1/b)Œ£·µ¢‚Çå‚ÇÅ·µá ‚àáF(x‚Çñ, Œæ‚Çñ·µ¢)

Trade-offs:
- Larger b: Lower variance, higher computational cost
- Smaller b: Higher variance, lower computational cost
- b = 1: Standard SGD
- b = n: Full gradient

Optimal batch size depends on:
- Problem structure
- Available parallelism
- Memory constraints

7.4 Adaptive Methods:
--------------------
AdaGrad: Adapt step size based on gradient history
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - (Œ±/‚àö(G‚Çñ + Œµ))‚àáf(x‚Çñ)

where G‚Çñ = Œ£·µ¢‚Çå‚ÇÅ·µè ‚àáf(x·µ¢)‚àáf(x·µ¢)·µÄ

RMSprop: Exponential moving average
G‚Çñ = Œ≥G‚Çñ‚Çã‚ÇÅ + (1-Œ≥)‚àáf(x‚Çñ)‚àáf(x‚Çñ)·µÄ

Adam: Combines momentum and adaptive step sizes
m‚Çñ = Œ≤‚ÇÅm‚Çñ‚Çã‚ÇÅ + (1-Œ≤‚ÇÅ)‚àáf(x‚Çñ)
v‚Çñ = Œ≤‚ÇÇv‚Çñ‚Çã‚ÇÅ + (1-Œ≤‚ÇÇ)[‚àáf(x‚Çñ)]¬≤
x‚Çñ‚Çä‚ÇÅ = x‚Çñ - Œ±(mÃÇ‚Çñ/‚àövÃÇ‚Çñ + Œµ)

where mÃÇ‚Çñ, vÃÇ‚Çñ are bias-corrected estimates

=======================================================

8. APPLICATIONS IN MACHINE LEARNING
===================================

8.1 Empirical Risk Minimization:
-------------------------------
Problem: minimize (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø ‚Ñì(f(x·µ¢; Œ∏), y·µ¢) + ŒªR(Œ∏)

Components:
- Loss function: ‚Ñì(≈∑, y)
- Model: f(x; Œ∏) parameterized by Œ∏
- Regularization: R(Œ∏) (e.g., ||Œ∏||¬≤, ||Œ∏||‚ÇÅ)
- Regularization parameter: Œª ‚â• 0

Gradient: ‚àáŒ∏ = (1/n)Œ£·µ¢‚Çå‚ÇÅ‚Åø ‚àá‚Ñì(f(x·µ¢; Œ∏), y·µ¢) + Œª‚àáR(Œ∏)

8.2 Neural Network Training:
---------------------------
Forward propagation: Compute activations layer by layer
Backward propagation: Compute gradients via chain rule

Challenges:
- High dimensionality (millions/billions of parameters)
- Non-convex optimization landscape
- Saddle points and local minima
- Vanishing/exploding gradients

Solutions:
- Stochastic optimization (SGD, Adam)
- Batch normalization
- Residual connections
- Careful initialization

8.3 Logistic Regression:
-----------------------
Objective: minimize Œ£·µ¢‚Çå‚ÇÅ‚Åø log(1 + exp(-y·µ¢x·µ¢·µÄŒ∏)) + Œª||Œ∏||¬≤

Properties:
- Convex objective function
- Smooth gradients
- Global optimum exists

Gradient: ‚àáŒ∏ = Œ£·µ¢‚Çå‚ÇÅ‚Åø (-y·µ¢x·µ¢)/(1 + exp(y·µ¢x·µ¢·µÄŒ∏)) + 2ŒªŒ∏

Methods:
- Gradient descent
- Newton-CG
- L-BFGS
- Stochastic methods for large datasets

8.4 Support Vector Machines:
---------------------------
Primal problem: minimize ¬Ω||w||¬≤ + C Œ£·µ¢‚Çå‚ÇÅ‚Åø max(0, 1 - y·µ¢(w·µÄx·µ¢ + b))

Challenges:
- Non-smooth objective (hinge loss)
- Requires subgradient methods
- Often solved via dual formulation

Smooth approximations:
- Huber loss
- Logistic loss
- Enable standard gradient methods

8.5 Matrix Factorization:
------------------------
Problem: minimize ||R - UV^T||¬≤ + Œª(||U||¬≤ + ||V||¬≤)

where R ‚àà ‚Ñù·µêÀ£‚Åø is observed matrix, U ‚àà ‚Ñù·µêÀ£·µè, V ‚àà ‚Ñù‚ÅøÀ£·µè

Alternating optimization:
- Fix V, optimize U (convex subproblem)
- Fix U, optimize V (convex subproblem)
- Repeat until convergence

Stochastic optimization:
- Sample observed entries randomly
- Update factors based on single entries
- Efficient for sparse matrices

8.6 Hyperparameter Optimization:
-------------------------------
Problem: minimize validation error as function of hyperparameters

Challenges:
- Expensive function evaluations (train full model)
- Non-convex, non-smooth objective
- Mixed continuous/discrete variables

Methods:
- Grid search
- Random search
- Bayesian optimization
- Gradient-based optimization (when possible)

8.7 Regularization Paths:
------------------------
Problem: Solve optimization for sequence of regularization parameters Œª

Examples:
- LASSO path: min ||y - XŒ≤||¬≤ + Œª||Œ≤||‚ÇÅ
- Ridge path: min ||y - XŒ≤||¬≤ + Œª||Œ≤||¬≤

Path-following algorithms:
- Start with large Œª (heavily regularized)
- Gradually decrease Œª
- Use previous solution as warm start

Benefits:
- Computational efficiency
- Model selection across regularization levels
- Understanding of solution structure

Key Insights for ML:
- Most ML problems are optimization problems
- Understanding convergence properties guides algorithm choice
- Stochastic methods essential for large-scale problems
- Second-order information valuable but expensive
- Convexity simplifies analysis but many ML problems non-convex
- Trade-offs between convergence speed and computational cost

=======================================================
END OF DOCUMENT 