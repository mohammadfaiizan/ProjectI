MODEL INTERPRETABILITY IN PRODUCTION
===================================

Table of Contents:
1. Interpretability Fundamentals
2. LIME (Local Interpretable Model-agnostic Explanations)
3. SHAP (SHapley Additive exPlanations)
4. Feature Importance in Production
5. Production Explainability Systems
6. Performance and Scalability
7. Regulatory and Compliance Considerations
8. Best Practices and Implementation

================================================================================
1. INTERPRETABILITY FUNDAMENTALS
================================================================================

1.1 Why Interpretability Matters in Production
----------------------------------------------
**Business Requirements:**
- Regulatory compliance (GDPR, Fair Credit Reporting Act)
- Building trust with stakeholders and customers
- Debugging model failures and edge cases
- Identifying bias and fairness issues
- Supporting business decision-making

**Technical Benefits:**
- Model debugging and validation
- Feature engineering insights
- Model improvement opportunities
- Error analysis and troubleshooting
- Monitoring for concept drift

1.2 Types of Interpretability
-----------------------------
**Global Interpretability:**
- Understanding model behavior across entire dataset
- Feature importance rankings
- Model-wide patterns and relationships

**Local Interpretability:**
- Understanding individual predictions
- Feature contributions for specific instances
- Counterfactual explanations

**Post-hoc vs. Intrinsic:**
- **Post-hoc:** Applied after model training (LIME, SHAP)
- **Intrinsic:** Built into model architecture (linear models, decision trees)

1.3 Production Interpretability Requirements
-------------------------------------------
**Performance Constraints:**
- Real-time explanation generation
- Scalable to high-volume predictions
- Minimal impact on inference latency

**Quality Requirements:**
- Consistent and stable explanations
- Faithful to actual model behavior
- Understandable by non-technical stakeholders

**Integration Requirements:**
- Seamless integration with existing ML pipelines
- API-compatible explanation services
- Monitoring and logging capabilities

================================================================================
2. LIME (LOCAL INTERPRETABLE MODEL-AGNOSTIC EXPLANATIONS)
================================================================================

2.1 LIME Theory and Implementation
----------------------------------
```python
import lime
import lime.lime_numpy
import lime.lime_tabular
import numpy as np
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

class ProductionLIME:
    def __init__(self, model, training_data, feature_names=None, mode='classification'):
        self.model = model
        self.training_data = training_data
        self.feature_names = feature_names or [f'feature_{i}' for i in range(training_data.shape[1])]
        self.mode = mode
        
        # Initialize LIME explainer
        if mode == 'classification':
            self.explainer = lime.lime_tabular.LimeTabularExplainer(
                training_data=training_data,
                feature_names=self.feature_names,
                class_names=['Class 0', 'Class 1'],  # Adjust based on your classes
                mode='classification',
                discretize_continuous=True
            )
        else:
            self.explainer = lime.lime_tabular.LimeTabularExplainer(
                training_data=training_data,
                feature_names=self.feature_names,
                mode='regression'
            )
    
    def explain_prediction(self, instance, num_features=10, num_samples=1000):
        """Generate LIME explanation for a single prediction"""
        
        try:
            # Generate explanation
            explanation = self.explainer.explain_instance(
                data_row=instance,
                predict_fn=self.model.predict_proba if self.mode == 'classification' else self.model.predict,
                num_features=num_features,
                num_samples=num_samples
            )
            
            # Extract feature importances
            feature_importances = explanation.as_list()
            
            # Create structured output
            result = {
                'prediction': self.model.predict([instance])[0],
                'feature_importances': [
                    {
                        'feature': feature,
                        'importance': importance,
                        'feature_value': self._get_feature_value(instance, feature)
                    }
                    for feature, importance in feature_importances
                ],
                'explanation_score': explanation.score,
                'local_accuracy': explanation.local_exp[1] if self.mode == 'classification' else None
            }
            
            if self.mode == 'classification':
                probabilities = self.model.predict_proba([instance])[0]
                result['prediction_probabilities'] = {
                    f'class_{i}': prob for i, prob in enumerate(probabilities)
                }
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'prediction': self.model.predict([instance])[0] if hasattr(self.model, 'predict') else None
            }
    
    def _get_feature_value(self, instance, feature_name):
        """Get actual feature value from instance"""
        if feature_name in self.feature_names:
            feature_index = self.feature_names.index(feature_name)
            return instance[feature_index]
        return None
    
    def batch_explain(self, instances, num_features=10, num_samples=1000):
        """Generate explanations for multiple instances"""
        
        explanations = []
        
        for i, instance in enumerate(instances):
            explanation = self.explain_prediction(
                instance, num_features, num_samples
            )
            explanation['instance_index'] = i
            explanations.append(explanation)
        
        return explanations
    
    def get_global_feature_importance(self, sample_size=1000, num_features=10):
        """Generate global feature importance using LIME sampling"""
        
        # Sample instances from training data
        sample_indices = np.random.choice(
            len(self.training_data), 
            size=min(sample_size, len(self.training_data)), 
            replace=False
        )
        
        feature_importance_sum = {}
        valid_explanations = 0
        
        for idx in sample_indices:
            instance = self.training_data[idx]
            explanation = self.explain_prediction(instance, num_features)
            
            if 'error' not in explanation:
                valid_explanations += 1
                for feature_data in explanation['feature_importances']:
                    feature = feature_data['feature']
                    importance = abs(feature_data['importance'])  # Use absolute importance
                    
                    if feature not in feature_importance_sum:
                        feature_importance_sum[feature] = 0
                    feature_importance_sum[feature] += importance
        
        # Average the importances
        if valid_explanations > 0:
            global_importance = {
                feature: importance / valid_explanations
                for feature, importance in feature_importance_sum.items()
            }
            
            # Sort by importance
            sorted_importance = sorted(
                global_importance.items(), 
                key=lambda x: x[1], 
                reverse=True
            )
            
            return {
                'global_feature_importance': sorted_importance,
                'sample_size': valid_explanations,
                'explanation_coverage': valid_explanations / len(sample_indices)
            }
        else:
            return {'error': 'No valid explanations generated'}

# Production-optimized LIME with caching
class CachedLIME(ProductionLIME):
    def __init__(self, model, training_data, feature_names=None, mode='classification', cache_size=1000):
        super().__init__(model, training_data, feature_names, mode)
        self.explanation_cache = {}
        self.cache_size = cache_size
        
    def explain_prediction(self, instance, num_features=10, num_samples=1000):
        """Generate LIME explanation with caching"""
        
        # Create cache key
        cache_key = self._create_cache_key(instance, num_features, num_samples)
        
        # Check cache
        if cache_key in self.explanation_cache:
            return self.explanation_cache[cache_key]
        
        # Generate explanation
        explanation = super().explain_prediction(instance, num_features, num_samples)
        
        # Cache result
        self._cache_explanation(cache_key, explanation)
        
        return explanation
    
    def _create_cache_key(self, instance, num_features, num_samples):
        """Create cache key for instance"""
        # Use hash of instance values and parameters
        instance_hash = hash(tuple(instance))
        return f"{instance_hash}_{num_features}_{num_samples}"
    
    def _cache_explanation(self, cache_key, explanation):
        """Cache explanation with LRU eviction"""
        if len(self.explanation_cache) >= self.cache_size:
            # Remove oldest entry
            oldest_key = next(iter(self.explanation_cache))
            del self.explanation_cache[oldest_key]
        
        self.explanation_cache[cache_key] = explanation
```

2.2 LIME for Different Data Types
---------------------------------
```python
class MultiModalLIME:
    def __init__(self, model):
        self.model = model
        self.explainers = {}
    
    def setup_tabular_explainer(self, training_data, feature_names, categorical_features=None):
        """Setup LIME explainer for tabular data"""
        self.explainers['tabular'] = lime.lime_tabular.LimeTabularExplainer(
            training_data=training_data,
            feature_names=feature_names,
            categorical_features=categorical_features,
            mode='classification'
        )
    
    def setup_text_explainer(self, class_names):
        """Setup LIME explainer for text data"""
        self.explainers['text'] = lime.lime_text.LimeTextExplainer(
            class_names=class_names,
            mode='classification'
        )
    
    def setup_image_explainer(self):
        """Setup LIME explainer for image data"""
        self.explainers['image'] = lime.lime_image.LimeImageExplainer()
    
    def explain_tabular(self, instance, num_features=10):
        """Explain tabular data prediction"""
        if 'tabular' not in self.explainers:
            raise ValueError("Tabular explainer not configured")
        
        explanation = self.explainers['tabular'].explain_instance(
            data_row=instance,
            predict_fn=self.model.predict_proba,
            num_features=num_features
        )
        
        return self._format_tabular_explanation(explanation)
    
    def explain_text(self, text_instance, num_features=10):
        """Explain text classification prediction"""
        if 'text' not in self.explainers:
            raise ValueError("Text explainer not configured")
        
        explanation = self.explainers['text'].explain_instance(
            text_instance=text_instance,
            classifier_fn=self.model.predict_proba,
            num_features=num_features
        )
        
        return self._format_text_explanation(explanation)
    
    def explain_image(self, image, num_features=100, hide_color=0):
        """Explain image classification prediction"""
        if 'image' not in self.explainers:
            raise ValueError("Image explainer not configured")
        
        explanation = self.explainers['image'].explain_instance(
            image=image,
            classifier_fn=self.model.predict_proba,
            top_labels=2,
            hide_color=hide_color,
            num_samples=1000
        )
        
        return self._format_image_explanation(explanation)
    
    def _format_tabular_explanation(self, explanation):
        """Format tabular explanation for API response"""
        return {
            'type': 'tabular',
            'feature_importances': explanation.as_list(),
            'score': explanation.score
        }
    
    def _format_text_explanation(self, explanation):
        """Format text explanation for API response"""
        return {
            'type': 'text',
            'word_importances': explanation.as_list(),
            'score': explanation.score
        }
    
    def _format_image_explanation(self, explanation):
        """Format image explanation for API response"""
        # This would include image segments and their importances
        return {
            'type': 'image',
            'segments': explanation.segments,
            'score': explanation.score
        }
```

================================================================================
3. SHAP (SHAPLEY ADDITIVE EXPLANATIONS)
================================================================================

3.1 SHAP Implementation for Production
--------------------------------------
```python
import shap
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

class ProductionSHAP:
    def __init__(self, model, background_data, model_type='tree'):
        self.model = model
        self.background_data = background_data
        self.model_type = model_type
        self.explainer = self._initialize_explainer()
        
    def _initialize_explainer(self):
        """Initialize appropriate SHAP explainer based on model type"""
        
        if self.model_type == 'tree':
            # For tree-based models (Random Forest, XGBoost, etc.)
            return shap.TreeExplainer(self.model)
            
        elif self.model_type == 'linear':
            # For linear models
            return shap.LinearExplainer(self.model, self.background_data)
            
        elif self.model_type == 'deep':
            # For deep learning models
            return shap.DeepExplainer(self.model, self.background_data)
            
        elif self.model_type == 'kernel':
            # Model-agnostic explainer (slower but works with any model)
            return shap.KernelExplainer(self.model.predict, self.background_data)
            
        else:
            # Default to kernel explainer
            return shap.KernelExplainer(self.model.predict, self.background_data)
    
    def explain_prediction(self, instance, feature_names=None):
        """Generate SHAP explanation for a single prediction"""
        
        try:
            # Ensure instance is in correct format
            if isinstance(instance, list):
                instance = np.array(instance).reshape(1, -1)
            elif len(instance.shape) == 1:
                instance = instance.reshape(1, -1)
            
            # Calculate SHAP values
            shap_values = self.explainer.shap_values(instance)
            
            # Handle different output formats
            if isinstance(shap_values, list):
                # Multi-class classification
                result = {
                    'prediction': self.model.predict(instance)[0],
                    'base_value': self.explainer.expected_value,
                    'shap_values': {}
                }
                
                if hasattr(self.model, 'predict_proba'):
                    probabilities = self.model.predict_proba(instance)[0]
                    result['prediction_probabilities'] = probabilities.tolist()
                
                for i, class_shap_values in enumerate(shap_values):
                    class_explanations = self._format_shap_values(
                        class_shap_values[0], feature_names, instance[0]
                    )
                    result['shap_values'][f'class_{i}'] = class_explanations
                    
            else:
                # Regression or binary classification
                result = {
                    'prediction': self.model.predict(instance)[0],
                    'base_value': self.explainer.expected_value,
                    'shap_values': self._format_shap_values(
                        shap_values[0], feature_names, instance[0]
                    )
                }
                
                if hasattr(self.model, 'predict_proba'):
                    probabilities = self.model.predict_proba(instance)[0]
                    result['prediction_probabilities'] = probabilities.tolist()
            
            return result
            
        except Exception as e:
            return {
                'error': str(e),
                'prediction': self.model.predict(instance)[0] if hasattr(self.model, 'predict') else None
            }
    
    def _format_shap_values(self, shap_values, feature_names, instance_values):
        """Format SHAP values for API response"""
        
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(len(shap_values))]
        
        formatted_values = []
        for i, (feature_name, shap_value) in enumerate(zip(feature_names, shap_values)):
            formatted_values.append({
                'feature': feature_name,
                'shap_value': float(shap_value),
                'feature_value': float(instance_values[i]),
                'abs_shap_value': abs(float(shap_value))
            })
        
        # Sort by absolute SHAP value
        formatted_values.sort(key=lambda x: x['abs_shap_value'], reverse=True)
        
        return formatted_values
    
    def batch_explain(self, instances, feature_names=None, batch_size=100):
        """Generate SHAP explanations for multiple instances"""
        
        explanations = []
        
        # Process in batches for memory efficiency
        for i in range(0, len(instances), batch_size):
            batch = instances[i:i+batch_size]
            
            try:
                # Calculate SHAP values for batch
                batch_shap_values = self.explainer.shap_values(batch)
                
                # Process each instance in batch
                for j, instance in enumerate(batch):
                    if isinstance(batch_shap_values, list):
                        # Multi-class
                        explanation = {
                            'instance_index': i + j,
                            'prediction': self.model.predict(instance.reshape(1, -1))[0],
                            'base_value': self.explainer.expected_value,
                            'shap_values': {}
                        }
                        
                        for k, class_shap_values in enumerate(batch_shap_values):
                            class_explanations = self._format_shap_values(
                                class_shap_values[j], feature_names, instance
                            )
                            explanation['shap_values'][f'class_{k}'] = class_explanations
                    else:
                        # Single output
                        explanation = {
                            'instance_index': i + j,
                            'prediction': self.model.predict(instance.reshape(1, -1))[0],
                            'base_value': self.explainer.expected_value,
                            'shap_values': self._format_shap_values(
                                batch_shap_values[j], feature_names, instance
                            )
                        }
                    
                    explanations.append(explanation)
                    
            except Exception as e:
                # Handle batch errors
                for j in range(len(batch)):
                    explanations.append({
                        'instance_index': i + j,
                        'error': str(e)
                    })
        
        return explanations
    
    def get_feature_importance(self, instances, feature_names=None):
        """Calculate global feature importance using SHAP values"""
        
        # Calculate SHAP values for all instances
        shap_values = self.explainer.shap_values(instances)
        
        if isinstance(shap_values, list):
            # Multi-class: average across classes
            combined_shap = np.mean([np.abs(sv) for sv in shap_values], axis=0)
        else:
            combined_shap = np.abs(shap_values)
        
        # Calculate mean absolute SHAP values per feature
        feature_importance = np.mean(combined_shap, axis=0)
        
        if feature_names is None:
            feature_names = [f'feature_{i}' for i in range(len(feature_importance))]
        
        # Create sorted list of feature importances
        importance_list = [
            {
                'feature': feature_name,
                'importance': float(importance)
            }
            for feature_name, importance in zip(feature_names, feature_importance)
        ]
        
        importance_list.sort(key=lambda x: x['importance'], reverse=True)
        
        return {
            'global_feature_importance': importance_list,
            'sample_size': len(instances)
        }

# High-performance SHAP with approximation
class ApproximateSHAP(ProductionSHAP):
    def __init__(self, model, background_data, model_type='tree', max_evals=100):
        super().__init__(model, background_data, model_type)
        self.max_evals = max_evals
        
    def _initialize_explainer(self):
        """Initialize explainer with performance optimizations"""
        
        if self.model_type == 'tree':
            return shap.TreeExplainer(
                self.model,
                feature_perturbation='interventional'  # Faster for tree models
            )
        elif self.model_type == 'kernel':
            return shap.KernelExplainer(
                self.model.predict,
                shap.sample(self.background_data, 100),  # Reduced background sample
                link="identity"
            )
        else:
            return super()._initialize_explainer()
    
    def explain_prediction(self, instance, feature_names=None):
        """Generate approximate SHAP explanation"""
        
        if self.model_type == 'kernel':
            # Use fewer evaluations for kernel explainer
            original_max_evals = getattr(self.explainer, 'max_evals', None)
            self.explainer.max_evals = self.max_evals
            
            result = super().explain_prediction(instance, feature_names)
            
            if original_max_evals is not None:
                self.explainer.max_evals = original_max_evals
                
            return result
        else:
            return super().explain_prediction(instance, feature_names)
```

3.2 SHAP Visualization for Production
-------------------------------------
```python
class SHAPVisualizer:
    def __init__(self, shap_explainer):
        self.explainer = shap_explainer
        
    def create_waterfall_data(self, shap_explanation):
        """Create data for waterfall visualization"""
        
        if 'shap_values' in shap_explanation:
            shap_values = shap_explanation['shap_values']
            
            # Handle multi-class case
            if isinstance(shap_values, dict):
                # Use first class for visualization
                first_class = list(shap_values.keys())[0]
                values = shap_values[first_class]
            else:
                values = shap_values
            
            waterfall_data = {
                'base_value': shap_explanation['base_value'],
                'prediction': shap_explanation['prediction'],
                'features': [
                    {
                        'name': item['feature'],
                        'value': item['feature_value'],
                        'shap_value': item['shap_value']
                    }
                    for item in values[:10]  # Top 10 features
                ]
            }
            
            return waterfall_data
        
        return None
    
    def create_summary_plot_data(self, batch_explanations, feature_names=None):
        """Create data for summary plot visualization"""
        
        # Extract SHAP values from batch explanations
        all_shap_values = []
        all_feature_values = []
        
        for explanation in batch_explanations:
            if 'error' not in explanation and 'shap_values' in explanation:
                shap_values = explanation['shap_values']
                
                if isinstance(shap_values, dict):
                    # Multi-class: use first class
                    first_class = list(shap_values.keys())[0]
                    values = shap_values[first_class]
                else:
                    values = shap_values
                
                shap_row = [item['shap_value'] for item in values]
                feature_row = [item['feature_value'] for item in values]
                
                all_shap_values.append(shap_row)
                all_feature_values.append(feature_row)
        
        if all_shap_values:
            return {
                'shap_values': all_shap_values,
                'feature_values': all_feature_values,
                'feature_names': feature_names or [f'feature_{i}' for i in range(len(all_shap_values[0]))]
            }
        
        return None
```

================================================================================
4. FEATURE IMPORTANCE IN PRODUCTION
================================================================================

4.1 Production Feature Importance Pipeline
------------------------------------------
```python
class ProductionFeatureImportance:
    def __init__(self, model, method='shap'):
        self.model = model
        self.method = method
        self.importance_cache = {}
        self.update_frequency = 3600  # Update every hour
        self.last_update = 0
        
    def get_feature_importance(self, background_data=None, force_update=False):
        """Get feature importance with caching"""
        
        current_time = time.time()
        
        # Check if update is needed
        if (force_update or 
            current_time - self.last_update > self.update_frequency or
            'importance' not in self.importance_cache):
            
            self.importance_cache['importance'] = self._calculate_importance(background_data)
            self.last_update = current_time
        
        return self.importance_cache['importance']
    
    def _calculate_importance(self, background_data):
        """Calculate feature importance using specified method"""
        
        if self.method == 'shap':
            return self._shap_importance(background_data)
        elif self.method == 'permutation':
            return self._permutation_importance(background_data)
        elif self.method == 'built_in':
            return self._built_in_importance()
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def _shap_importance(self, background_data):
        """Calculate SHAP-based feature importance"""
        
        if background_data is None:
            raise ValueError("Background data required for SHAP importance")
        
        # Sample data for efficiency
        sample_size = min(1000, len(background_data))
        sample_data = background_data.sample(sample_size) if hasattr(background_data, 'sample') else background_data[:sample_size]
        
        # Initialize SHAP explainer
        shap_explainer = ProductionSHAP(self.model, sample_data[:100])  # Smaller background for speed
        
        # Calculate importance
        importance_result = shap_explainer.get_feature_importance(sample_data)
        
        return {
            'method': 'shap',
            'importances': importance_result['global_feature_importance'],
            'sample_size': importance_result['sample_size']
        }
    
    def _permutation_importance(self, background_data):
        """Calculate permutation-based feature importance"""
        
        from sklearn.inspection import permutation_importance
        
        if background_data is None:
            raise ValueError("Background data required for permutation importance")
        
        # Assume last column is target (adjust as needed)
        X = background_data.iloc[:, :-1] if hasattr(background_data, 'iloc') else background_data[:, :-1]
        y = background_data.iloc[:, -1] if hasattr(background_data, 'iloc') else background_data[:, -1]
        
        # Calculate permutation importance
        result = permutation_importance(
            self.model, X, y,
            n_repeats=10,
            random_state=42,
            n_jobs=-1
        )
        
        # Format results
        feature_names = X.columns if hasattr(X, 'columns') else [f'feature_{i}' for i in range(X.shape[1])]
        
        importances = [
            {
                'feature': feature_name,
                'importance': float(importance_mean),
                'std': float(importance_std)
            }
            for feature_name, importance_mean, importance_std in 
            zip(feature_names, result.importances_mean, result.importances_std)
        ]
        
        # Sort by importance
        importances.sort(key=lambda x: x['importance'], reverse=True)
        
        return {
            'method': 'permutation',
            'importances': importances,
            'sample_size': len(X)
        }
    
    def _built_in_importance(self):
        """Use model's built-in feature importance"""
        
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            
            # Get feature names
            feature_names = getattr(self.model, 'feature_names_', 
                                  [f'feature_{i}' for i in range(len(importances))])
            
            importance_list = [
                {
                    'feature': feature_name,
                    'importance': float(importance)
                }
                for feature_name, importance in zip(feature_names, importances)
            ]
            
            importance_list.sort(key=lambda x: x['importance'], reverse=True)
            
            return {
                'method': 'built_in',
                'importances': importance_list,
                'model_type': type(self.model).__name__
            }
        
        elif hasattr(self.model, 'coef_'):
            # Linear model coefficients
            coefficients = self.model.coef_
            
            if len(coefficients.shape) > 1:
                # Multi-class: use first class or average
                coefficients = coefficients[0] if coefficients.shape[0] == 1 else np.mean(np.abs(coefficients), axis=0)
            
            feature_names = getattr(self.model, 'feature_names_', 
                                  [f'feature_{i}' for i in range(len(coefficients))])
            
            importance_list = [
                {
                    'feature': feature_name,
                    'importance': float(abs(coef))
                }
                for feature_name, coef in zip(feature_names, coefficients)
            ]
            
            importance_list.sort(key=lambda x: x['importance'], reverse=True)
            
            return {
                'method': 'linear_coefficients',
                'importances': importance_list,
                'model_type': type(self.model).__name__
            }
        
        else:
            raise ValueError("Model does not have built-in feature importance")

# Feature importance comparison
class FeatureImportanceComparison:
    def __init__(self, model, background_data):
        self.model = model
        self.background_data = background_data
        
    def compare_methods(self, methods=['shap', 'permutation', 'built_in']):
        """Compare feature importance across different methods"""
        
        results = {}
        
        for method in methods:
            try:
                importance_calculator = ProductionFeatureImportance(self.model, method)
                result = importance_calculator.get_feature_importance(self.background_data)
                results[method] = result
            except Exception as e:
                results[method] = {'error': str(e)}
        
        # Calculate correlation between methods
        correlations = self._calculate_method_correlations(results)
        
        return {
            'method_results': results,
            'method_correlations': correlations,
            'consensus_ranking': self._calculate_consensus_ranking(results)
        }
    
    def _calculate_method_correlations(self, results):
        """Calculate correlation between different importance methods"""
        
        correlations = {}
        valid_methods = [method for method, result in results.items() if 'error' not in result]
        
        if len(valid_methods) < 2:
            return correlations
        
        # Extract importance values for correlation
        method_importances = {}
        common_features = None
        
        for method in valid_methods:
            importances = results[method]['importances']
            importance_dict = {item['feature']: item['importance'] for item in importances}
            method_importances[method] = importance_dict
            
            if common_features is None:
                common_features = set(importance_dict.keys())
            else:
                common_features = common_features.intersection(set(importance_dict.keys()))
        
        # Calculate pairwise correlations
        common_features = list(common_features)
        
        for i, method1 in enumerate(valid_methods):
            for method2 in valid_methods[i+1:]:
                values1 = [method_importances[method1][feature] for feature in common_features]
                values2 = [method_importances[method2][feature] for feature in common_features]
                
                correlation = np.corrcoef(values1, values2)[0, 1]
                correlations[f"{method1}_vs_{method2}"] = float(correlation)
        
        return correlations
    
    def _calculate_consensus_ranking(self, results):
        """Calculate consensus feature ranking across methods"""
        
        valid_methods = [method for method, result in results.items() if 'error' not in result]
        
        if not valid_methods:
            return []
        
        # Collect feature rankings from each method
        feature_ranks = {}
        
        for method in valid_methods:
            importances = results[method]['importances']
            for rank, item in enumerate(importances):
                feature = item['feature']
                if feature not in feature_ranks:
                    feature_ranks[feature] = []
                feature_ranks[feature].append(rank + 1)  # 1-based ranking
        
        # Calculate average rank for each feature
        consensus_ranking = []
        for feature, ranks in feature_ranks.items():
            avg_rank = np.mean(ranks)
            consensus_ranking.append({
                'feature': feature,
                'average_rank': float(avg_rank),
                'rank_std': float(np.std(ranks)),
                'methods_count': len(ranks)
            })
        
        # Sort by average rank
        consensus_ranking.sort(key=lambda x: x['average_rank'])
        
        return consensus_ranking
```

================================================================================
5. PRODUCTION EXPLAINABILITY SYSTEMS
================================================================================

5.1 Explainability API Service
------------------------------
```python
from flask import Flask, request, jsonify
import time
import threading

class ExplainabilityAPI:
    def __init__(self, model, explainers_config):
        self.app = Flask(__name__)
        self.model = model
        self.explainers = {}
        self.request_cache = {}
        self.cache_lock = threading.Lock()
        
        self._initialize_explainers(explainers_config)
        self._setup_routes()
    
    def _initialize_explainers(self, config):
        """Initialize different explainer types"""
        
        if 'lime' in config:
            self.explainers['lime'] = ProductionLIME(
                model=self.model,
                training_data=config['lime']['background_data'],
                feature_names=config['lime'].get('feature_names'),
                mode=config['lime'].get('mode', 'classification')
            )
        
        if 'shap' in config:
            self.explainers['shap'] = ProductionSHAP(
                model=self.model,
                background_data=config['shap']['background_data'],
                model_type=config['shap'].get('model_type', 'tree')
            )
        
        if 'feature_importance' in config:
            self.explainers['feature_importance'] = ProductionFeatureImportance(
                model=self.model,
                method=config['feature_importance'].get('method', 'shap')
            )
    
    def _setup_routes(self):
        """Setup API routes"""
        
        @self.app.route('/explain', methods=['POST'])
        def explain():
            """Generate explanation for single prediction"""
            
            try:
                data = request.get_json()
                
                # Validate input
                if 'instance' not in data:
                    return jsonify({'error': 'Missing instance data'}), 400
                
                instance = data['instance']
                explainer_type = data.get('explainer', 'shap')
                feature_names = data.get('feature_names')
                
                # Check cache
                cache_key = self._create_cache_key(instance, explainer_type)
                cached_result = self._get_cached_result(cache_key)
                
                if cached_result:
                    return jsonify(cached_result)
                
                # Generate explanation
                if explainer_type not in self.explainers:
                    return jsonify({'error': f'Explainer {explainer_type} not available'}), 400
                
                explainer = self.explainers[explainer_type]
                
                start_time = time.time()
                explanation = explainer.explain_prediction(instance, feature_names=feature_names)
                explanation_time = time.time() - start_time
                
                # Add metadata
                explanation['metadata'] = {
                    'explainer_type': explainer_type,
                    'explanation_time_ms': explanation_time * 1000,
                    'timestamp': time.time()
                }
                
                # Cache result
                self._cache_result(cache_key, explanation)
                
                return jsonify(explanation)
                
            except Exception as e:
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/batch_explain', methods=['POST'])
        def batch_explain():
            """Generate explanations for multiple predictions"""
            
            try:
                data = request.get_json()
                
                if 'instances' not in data:
                    return jsonify({'error': 'Missing instances data'}), 400
                
                instances = data['instances']
                explainer_type = data.get('explainer', 'shap')
                feature_names = data.get('feature_names')
                
                if explainer_type not in self.explainers:
                    return jsonify({'error': f'Explainer {explainer_type} not available'}), 400
                
                explainer = self.explainers[explainer_type]
                
                start_time = time.time()
                
                if hasattr(explainer, 'batch_explain'):
                    explanations = explainer.batch_explain(instances, feature_names=feature_names)
                else:
                    # Fallback to individual explanations
                    explanations = []
                    for i, instance in enumerate(instances):
                        explanation = explainer.explain_prediction(instance, feature_names=feature_names)
                        explanation['instance_index'] = i
                        explanations.append(explanation)
                
                explanation_time = time.time() - start_time
                
                result = {
                    'explanations': explanations,
                    'metadata': {
                        'explainer_type': explainer_type,
                        'batch_size': len(instances),
                        'explanation_time_ms': explanation_time * 1000,
                        'timestamp': time.time()
                    }
                }
                
                return jsonify(result)
                
            except Exception as e:
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/feature_importance', methods=['GET'])
        def feature_importance():
            """Get global feature importance"""
            
            try:
                method = request.args.get('method', 'shap')
                force_update = request.args.get('force_update', 'false').lower() == 'true'
                
                if 'feature_importance' not in self.explainers:
                    return jsonify({'error': 'Feature importance explainer not available'}), 400
                
                importance_result = self.explainers['feature_importance'].get_feature_importance(
                    force_update=force_update
                )
                
                return jsonify(importance_result)
                
            except Exception as e:
                return jsonify({'error': str(e)}), 500
        
        @self.app.route('/health', methods=['GET'])
        def health():
            """Health check endpoint"""
            
            return jsonify({
                'status': 'healthy',
                'available_explainers': list(self.explainers.keys()),
                'timestamp': time.time()
            })
    
    def _create_cache_key(self, instance, explainer_type):
        """Create cache key for explanation"""
        instance_hash = hash(tuple(instance) if isinstance(instance, list) else tuple(instance.flatten()))
        return f"{explainer_type}_{instance_hash}"
    
    def _get_cached_result(self, cache_key):
        """Get cached explanation result"""
        with self.cache_lock:
            cached = self.request_cache.get(cache_key)
            if cached and time.time() - cached['timestamp'] < 3600:  # 1 hour cache
                return cached['result']
            return None
    
    def _cache_result(self, cache_key, result):
        """Cache explanation result"""
        with self.cache_lock:
            # Simple LRU cache implementation
            if len(self.request_cache) > 1000:  # Max cache size
                oldest_key = min(self.request_cache.keys(), 
                               key=lambda k: self.request_cache[k]['timestamp'])
                del self.request_cache[oldest_key]
            
            self.request_cache[cache_key] = {
                'result': result,
                'timestamp': time.time()
            }
    
    def run(self, host='0.0.0.0', port=5000, debug=False):
        """Run the explainability API server"""
        self.app.run(host=host, port=port, debug=debug)

# Usage example
explainers_config = {
    'lime': {
        'background_data': training_data.values,
        'feature_names': training_data.columns.tolist(),
        'mode': 'classification'
    },
    'shap': {
        'background_data': training_data.values,
        'model_type': 'tree'
    },
    'feature_importance': {
        'method': 'shap'
    }
}

api = ExplainabilityAPI(trained_model, explainers_config)
# api.run(debug=True)
```

================================================================================
6. PERFORMANCE AND SCALABILITY
================================================================================

6.1 Performance Optimization Strategies
---------------------------------------
```python
class OptimizedExplainer:
    def __init__(self, model, optimization_config):
        self.model = model
        self.config = optimization_config
        self.explainer_pool = {}
        
    def get_optimized_explanation(self, instance, explainer_type='auto', latency_budget_ms=1000):
        """Get explanation within latency budget"""
        
        if explainer_type == 'auto':
            explainer_type = self._select_optimal_explainer(instance, latency_budget_ms)
        
        start_time = time.time()
        
        try:
            if explainer_type == 'fast_approximation':
                explanation = self._fast_approximation_explanation(instance)
            elif explainer_type == 'cached_lime':
                explanation = self._cached_lime_explanation(instance)
            elif explainer_type == 'tree_shap':
                explanation = self._tree_shap_explanation(instance)
            else:
                explanation = self._fallback_explanation(instance)
            
            elapsed_time = (time.time() - start_time) * 1000
            explanation['performance'] = {
                'explanation_time_ms': elapsed_time,
                'explainer_used': explainer_type,
                'within_budget': elapsed_time <= latency_budget_ms
            }
            
            return explanation
            
        except Exception as e:
            return {
                'error': str(e),
                'explainer_used': explainer_type,
                'performance': {
                    'explanation_time_ms': (time.time() - start_time) * 1000
                }
            }
    
    def _select_optimal_explainer(self, instance, latency_budget_ms):
        """Select optimal explainer based on latency budget"""
        
        if latency_budget_ms < 100:
            return 'fast_approximation'
        elif latency_budget_ms < 500:
            return 'cached_lime'
        elif latency_budget_ms < 1000:
            return 'tree_shap'
        else:
            return 'full_shap'
    
    def _fast_approximation_explanation(self, instance):
        """Fast approximation using model-specific methods"""
        
        if hasattr(self.model, 'feature_importances_'):
            # Tree-based model: use built-in importance
            importance = self.model.feature_importances_
            
            # Scale importance by feature values
            scaled_importance = importance * np.abs(instance)
            
            feature_explanations = [
                {
                    'feature': f'feature_{i}',
                    'importance': float(imp),
                    'feature_value': float(instance[i])
                }
                for i, imp in enumerate(scaled_importance)
            ]
            
            # Sort by importance
            feature_explanations.sort(key=lambda x: abs(x['importance']), reverse=True)
            
            return {
                'prediction': self.model.predict([instance])[0],
                'feature_importances': feature_explanations[:10],  # Top 10
                'method': 'fast_approximation'
            }
        
        else:
            # Fallback to simple gradient-based explanation
            return self._gradient_explanation(instance)
    
    def _gradient_explanation(self, instance):
        """Simple gradient-based explanation"""
        
        # This is a simplified implementation
        # In practice, you'd compute actual gradients for neural networks
        
        perturbation = 0.01
        base_prediction = self.model.predict([instance])[0]
        
        feature_gradients = []
        
        for i in range(len(instance)):
            perturbed_instance = instance.copy()
            perturbed_instance[i] += perturbation
            
            perturbed_prediction = self.model.predict([perturbed_instance])[0]
            gradient = (perturbed_prediction - base_prediction) / perturbation
            
            feature_gradients.append({
                'feature': f'feature_{i}',
                'gradient': float(gradient),
                'feature_value': float(instance[i])
            })
        
        # Sort by absolute gradient
        feature_gradients.sort(key=lambda x: abs(x['gradient']), reverse=True)
        
        return {
            'prediction': base_prediction,
            'feature_gradients': feature_gradients[:10],
            'method': 'gradient_approximation'
        }

# Parallel explanation processing
class ParallelExplainer:
    def __init__(self, explainer, n_workers=4):
        self.explainer = explainer
        self.n_workers = n_workers
        
    def batch_explain_parallel(self, instances, feature_names=None):
        """Generate explanations in parallel"""
        
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        def explain_single(instance_with_index):
            index, instance = instance_with_index
            explanation = self.explainer.explain_prediction(instance, feature_names=feature_names)
            explanation['instance_index'] = index
            return explanation
        
        # Create list of (index, instance) pairs
        indexed_instances = list(enumerate(instances))
        
        explanations = []
        
        with ThreadPoolExecutor(max_workers=self.n_workers) as executor:
            # Submit all tasks
            future_to_instance = {
                executor.submit(explain_single, instance_with_index): instance_with_index
                for instance_with_index in indexed_instances
            }
            
            # Collect results
            for future in as_completed(future_to_instance):
                try:
                    explanation = future.result()
                    explanations.append(explanation)
                except Exception as e:
                    instance_with_index = future_to_instance[future]
                    explanations.append({
                        'instance_index': instance_with_index[0],
                        'error': str(e)
                    })
        
        # Sort by instance index
        explanations.sort(key=lambda x: x['instance_index'])
        
        return explanations
```

================================================================================
7. REGULATORY AND COMPLIANCE CONSIDERATIONS
================================================================================

7.1 Compliance Framework
------------------------
```python
class ComplianceExplainer:
    def __init__(self, model, compliance_requirements):
        self.model = model
        self.requirements = compliance_requirements
        self.audit_log = []
        
    def generate_compliant_explanation(self, instance, user_context=None):
        """Generate explanation that meets compliance requirements"""
        
        explanation_request = {
            'timestamp': time.time(),
            'user_context': user_context,
            'instance_hash': hash(tuple(instance)),
            'compliance_level': self.requirements.get('level', 'standard')
        }
        
        # Determine required explanation depth
        if self.requirements.get('level') == 'high':
            explanation = self._comprehensive_explanation(instance)
        elif self.requirements.get('level') == 'medium':
            explanation = self._standard_explanation(instance)
        else:
            explanation = self._basic_explanation(instance)
        
        # Add compliance metadata
        explanation['compliance'] = {
            'gdpr_compliant': self._check_gdpr_compliance(explanation),
            'fair_credit_compliant': self._check_fair_credit_compliance(explanation),
            'explanation_depth': self.requirements.get('level', 'standard'),
            'audit_trail_id': len(self.audit_log)
        }
        
        # Log for audit trail
        explanation_request['explanation'] = explanation
        self.audit_log.append(explanation_request)
        
        return explanation
    
    def _comprehensive_explanation(self, instance):
        """Generate comprehensive explanation for high compliance requirements"""
        
        # Use both LIME and SHAP for robustness
        lime_explainer = ProductionLIME(self.model, self.requirements['background_data'])
        shap_explainer = ProductionSHAP(self.model, self.requirements['background_data'])
        
        lime_explanation = lime_explainer.explain_prediction(instance)
        shap_explanation = shap_explainer.explain_prediction(instance)
        
        return {
            'prediction': self.model.predict([instance])[0],
            'lime_explanation': lime_explanation,
            'shap_explanation': shap_explanation,
            'model_type': type(self.model).__name__,
            'explanation_methods': ['LIME', 'SHAP'],
            'confidence_level': 'high'
        }
    
    def _check_gdpr_compliance(self, explanation):
        """Check if explanation meets GDPR requirements"""
        
        required_elements = [
            'prediction' in explanation,
            'explanation_methods' in explanation or 'feature_importances' in explanation,
            'model_type' in explanation
        ]
        
        return all(required_elements)
    
    def _check_fair_credit_compliance(self, explanation):
        """Check if explanation meets Fair Credit Reporting Act requirements"""
        
        # FCRA requires specific adverse action notices
        if self.requirements.get('domain') == 'credit':
            required_elements = [
                'prediction' in explanation,
                'feature_importances' in explanation or 'shap_values' in explanation,
                len(explanation.get('feature_importances', [])) >= 4  # At least 4 factors
            ]
            
            return all(required_elements)
        
        return True
    
    def generate_audit_report(self, time_range_hours=24):
        """Generate audit report for compliance review"""
        
        cutoff_time = time.time() - (time_range_hours * 3600)
        recent_requests = [
            req for req in self.audit_log
            if req['timestamp'] > cutoff_time
        ]
        
        compliance_stats = {
            'total_requests': len(recent_requests),
            'gdpr_compliant': sum(1 for req in recent_requests 
                                if req['explanation']['compliance']['gdpr_compliant']),
            'fair_credit_compliant': sum(1 for req in recent_requests 
                                       if req['explanation']['compliance']['fair_credit_compliant']),
            'explanation_levels': {}
        }
        
        # Count explanation levels
        for req in recent_requests:
            level = req['explanation']['compliance']['explanation_depth']
            compliance_stats['explanation_levels'][level] = compliance_stats['explanation_levels'].get(level, 0) + 1
        
        return {
            'audit_period_hours': time_range_hours,
            'compliance_statistics': compliance_stats,
            'sample_explanations': recent_requests[:5]  # Sample for review
        }

# Right to explanation handler
class RightToExplanationHandler:
    def __init__(self, explainer, user_data_store):
        self.explainer = explainer
        self.user_data_store = user_data_store
        
    def handle_explanation_request(self, user_id, decision_id):
        """Handle user's right to explanation request"""
        
        # Retrieve decision details
        decision_details = self.user_data_store.get_decision(user_id, decision_id)
        
        if not decision_details:
            return {'error': 'Decision not found'}
        
        # Generate user-friendly explanation
        explanation = self._generate_user_explanation(decision_details)
        
        # Log the explanation request
        self.user_data_store.log_explanation_request(user_id, decision_id, explanation)
        
        return explanation
    
    def _generate_user_explanation(self, decision_details):
        """Generate user-friendly explanation"""
        
        technical_explanation = decision_details['technical_explanation']
        
        # Simplify technical explanation for user consumption
        user_explanation = {
            'decision': decision_details['decision'],
            'key_factors': self._simplify_factors(technical_explanation.get('feature_importances', [])),
            'how_to_improve': self._generate_improvement_suggestions(technical_explanation),
            'explanation_date': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
            'your_rights': self._get_user_rights_information()
        }
        
        return user_explanation
    
    def _simplify_factors(self, feature_importances):
        """Convert technical feature names to user-friendly descriptions"""
        
        # This would be customized based on your domain
        feature_mapping = {
            'credit_score': 'Your credit score',
            'annual_income': 'Your annual income',
            'debt_to_income': 'Your debt-to-income ratio',
            'employment_length': 'Length of employment',
            # Add more mappings as needed
        }
        
        simplified_factors = []
        
        for factor in feature_importances[:5]:  # Top 5 factors
            feature_name = factor['feature']
            friendly_name = feature_mapping.get(feature_name, feature_name)
            
            simplified_factors.append({
                'factor': friendly_name,
                'impact': 'positive' if factor['importance'] > 0 else 'negative',
                'importance_level': 'high' if abs(factor['importance']) > 0.1 else 'medium'
            })
        
        return simplified_factors
```

================================================================================
8. BEST PRACTICES AND IMPLEMENTATION
================================================================================

8.1 Production Deployment Best Practices
----------------------------------------
```python
class ExplainabilityBestPractices:
    @staticmethod
    def get_implementation_guidelines():
        return {
            'performance_optimization': [
                'Cache explanations for repeated requests',
                'Use approximate methods for real-time requirements',
                'Implement parallel processing for batch explanations',
                'Optimize background data size for SHAP/LIME',
                'Monitor explanation generation latency'
            ],
            'accuracy_and_reliability': [
                'Validate explanations against known cases',
                'Use multiple explanation methods for critical decisions',
                'Implement consistency checks across explanations',
                'Regular model-explanation alignment testing',
                'Monitor explanation stability over time'
            ],
            'user_experience': [
                'Provide different explanation levels for different users',
                'Use domain-specific language and terminology',
                'Include visual representations where appropriate',
                'Offer interactive explanation exploration',
                'Provide actionable insights and recommendations'
            ],
            'compliance_and_governance': [
                'Maintain audit trails for all explanations',
                'Implement user consent management',
                'Regular compliance review and validation',
                'Clear data retention and deletion policies',
                'Staff training on explanation requirements'
            ]
        }
    
    @staticmethod
    def get_monitoring_checklist():
        return {
            'explanation_quality': [
                'Monitor explanation consistency across similar instances',
                'Track explanation generation success rates',
                'Validate explanation faithfulness to model behavior',
                'Monitor for explanation drift over time',
                'Regular human expert review of explanations'
            ],
            'performance_metrics': [
                'Explanation generation latency percentiles',
                'Cache hit rates and effectiveness',
                'Resource utilization for explanation services',
                'Explanation API availability and error rates',
                'Batch processing throughput and efficiency'
            ],
            'business_impact': [
                'User satisfaction with explanations',
                'Impact on decision appeal rates',
                'Regulatory audit outcomes',
                'Business process efficiency improvements',
                'Trust and adoption metrics'
            ]
        }

# Complete implementation example
EXPLAINABILITY_IMPLEMENTATION_EXAMPLE = {
    'architecture': {
        'components': [
            'Explanation API Service',
            'Multiple Explainer Backends (LIME, SHAP)',
            'Caching Layer (Redis)',
            'Audit Logging System',
            'User Interface for Explanations',
            'Monitoring and Alerting'
        ],
        'data_flow': 'Request  Cache Check  Explainer  Response + Audit Log'
    },
    'deployment_strategy': {
        'phase_1': 'Deploy explanation API alongside existing model serving',
        'phase_2': 'Integrate explanations into user-facing applications',
        'phase_3': 'Add advanced features (interactive explanations, what-if analysis)',
        'phase_4': 'Optimize for scale and performance'
    },
    'success_metrics': [
        'Explanation generation latency < 500ms for 95% of requests',
        'Explanation accuracy validated against expert review',
        'User satisfaction score > 4.0/5.0',
        'Regulatory compliance score 100%',
        'System availability > 99.9%'
    ]
}

================================================================================
SUMMARY AND KEY TAKEAWAYS
================================================================================

Model interpretability in production requires balancing accuracy, performance, and usability:

**Key Technologies:**
- **LIME:** Model-agnostic local explanations through local approximation
- **SHAP:** Theoretically grounded explanations based on game theory
- **Feature Importance:** Global understanding of model behavior
- **Domain-Specific Methods:** Tailored explanations for specific use cases

**Production Considerations:**
- **Performance:** Optimize for real-time explanation generation
- **Scalability:** Handle high-volume explanation requests efficiently
- **Caching:** Implement intelligent caching for repeated requests
- **Reliability:** Ensure consistent and stable explanations

**Compliance Requirements:**
- **GDPR:** Right to explanation for automated decision-making
- **Fair Credit Reporting Act:** Specific requirements for credit decisions
- **Industry Regulations:** Domain-specific explanation requirements
- **Audit Trails:** Comprehensive logging for regulatory review

**Implementation Strategy:**
- Start with post-hoc explanation methods (LIME, SHAP)
- Implement appropriate caching and optimization strategies
- Design user-friendly explanation interfaces
- Ensure compliance with relevant regulations
- Monitor explanation quality and system performance

**Best Practices:**
- Use multiple explanation methods for critical decisions
- Optimize explanation generation for production latency requirements
- Implement comprehensive audit logging and compliance checking
- Design explanations for the target audience (technical vs. end-user)
- Regular validation of explanation accuracy and consistency

**Success Factors:**
- Clear requirements for explanation quality and performance
- Appropriate technology selection based on model type and constraints
- Comprehensive testing and validation procedures
- User-centered design for explanation interfaces
- Continuous monitoring and improvement of explanation systems

Effective production interpretability enables trust, compliance, and better decision-making while maintaining the performance and reliability required for production ML systems. 